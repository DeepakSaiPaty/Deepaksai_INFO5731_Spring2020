{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of In-class-exercise-04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepakSaiPaty/Deepaksai_INFO5731_Spring2020/blob/master/Copy_of_In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuX00KHNeSpw",
        "colab_type": "text"
      },
      "source": [
        "# **The fourth in-class-exercise**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vTOb03hG1f",
        "colab_type": "text"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/INFO5731_Spring2020/blob/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above.\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 2-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR0L3_CreM_A",
        "colab_type": "code",
        "outputId": "c6a4a0c5-fbca-4ffb-ee30-c7fbb64d3bc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Write your code here\n",
        "pip install pysbd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pysbd\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/49/4799b3cdf80aee5fa4562a3929eda738845900bbeef4ee60481196ad4d1a/pysbd-0.2.3-py3-none-any.whl\n",
            "Installing collected packages: pysbd\n",
            "Successfully installed pysbd-0.2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueralD12jZ4j",
        "colab_type": "code",
        "outputId": "146778d9-79d0-4df1-b631-70c5c1510400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "pip install spacy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.9)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BfRDvsvjhCj",
        "colab_type": "code",
        "outputId": "7fb2406d-ab2a-4fba-bf4f-14308cdf4e4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#1.1\n",
        "import spacy\n",
        "import pysbd            #calculation of sentences\n",
        "f = open(\"data.txt\", \"r\")\n",
        "data=f.read()\n",
        "seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
        "sentences=seg.segment(data)\n",
        "print('Total Number of Sentences:',len(sentences))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of Sentences: 220\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLCY_dYWjkmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1.1\n",
        "import string             #Removed Punctuation\n",
        "import regex as re\n",
        "import pandas as pd\n",
        "noise_df=pd.DataFrame(sentences,columns=['Data'])\n",
        "df= noise_df['Data'].apply(lambda x:''.join([i for i in x if i not in string.punctuation])).to_frame()\n",
        "df2= df['Data'].map(lambda x: re.sub(r'\\W+', ' ', x))\n",
        "cleaned_list=df2.values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5hg97bzkYkS",
        "colab_type": "code",
        "outputId": "762cd182-2566-450b-c2e6-3ab31562f740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#1.1\n",
        "import re \n",
        "def remove(list):                   #Removed numbers from data and calculated words\n",
        "    pattern = '[0-9]'\n",
        "    list = [re.sub(pattern, '', i) for i in list] \n",
        "    return list\n",
        "words=remove(cleaned_list)\n",
        "number_of_words=[]\n",
        "for i in words:\n",
        "  res = len(re.findall(r'\\w+',i)) \n",
        "  number_of_words.append(res)\n",
        "print('Total Number of words:',sum(number_of_words))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Number of words: 3540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBIAFLsdpUOo",
        "colab_type": "code",
        "outputId": "7a1ad9d1-2cac-4b88-b9a9-78b763bb35e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#1.1\n",
        "no_space_cleaned= df['Data'].map(lambda x: re.sub(r'\\W+', '', x))\n",
        "no_space_l=no_space_cleaned.values.tolist()     #Removed spaces from data and calculated number of characters\n",
        "chars_only=remove(no_space_l)\n",
        "chars_only\n",
        "def count_chars(txt):\n",
        "\tresult = 0\n",
        "\tfor char in txt:\n",
        "\t\tresult += 1     \n",
        "\treturn result\n",
        "characters=[]\n",
        "for i in chars_only:\n",
        "  characters.append(count_chars(i))\n",
        "print('Total number of characters-',sum(characters))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of characters- 15547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0Eq5VqPvA1F",
        "colab_type": "code",
        "outputId": "f51cae9b-8c35-4c10-9b67-e140308b1b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#1.1\n",
        "average_l=[]      #Average word length\n",
        "for j in words:\n",
        "  try:\n",
        "    w = j.split()\n",
        "    average = sum(len(word) for word in w) / len(w)\n",
        "    average_l.append(average)\n",
        "  except:\n",
        "    continue; #If there are no words\n",
        "print('Average word count in document-',sum(average_l)/len(average_l))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average word count in document- 4.552546510966314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI8Xb_tZ8P58",
        "colab_type": "code",
        "outputId": "e0e0c2ce-0623-4df1-9f01-60abb3fefdb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n",
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       |   Unzipping corpora/omw.zip.\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet.zip.\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | \n",
            "     Done downloading collection all\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_qATofU3VPi",
        "colab_type": "code",
        "outputId": "802b8ef0-d5ed-40d8-8583-bdc944a31100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#1.1\n",
        "import nltk\n",
        "from nltk.corpus import stopwords         #calculating stop words\n",
        "from nltk.tokenize import word_tokenize\n",
        "lower_words=[]\n",
        "count=[]\n",
        "for i in words:\n",
        "  lower_words.append(i.lower())\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  word_tokens = word_tokenize(i) \n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "  filtered_sentence = [] \n",
        "  for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "  count.append(len(word_tokens)-len(filtered_sentence))\n",
        "print('Total stop Words-',sum(count))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total stop Words- 1732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRXwt1T8AJzm",
        "colab_type": "code",
        "outputId": "24b6e8e2-e468-4bb5-9203-130e5717b910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "#1.1\n",
        "alphabets = digits = special = 0\n",
        "digits_l=[]\n",
        "specials=[]\n",
        "upper_case=[]\n",
        "for s in sentences:\n",
        "  for i in range(len(s)):\n",
        "    if(s[i].isalpha()):\n",
        "      if(s[i].isupper()):\n",
        "        upper_case.append(s[i])\n",
        "      continue\n",
        "    elif(s[i].isdigit()):\n",
        "      digits_l.append(s[i])\n",
        "      digits = digits + 1\n",
        "    elif(s[i]==' '):\n",
        "      continue\n",
        "    else:\n",
        "      specials.append(s[i])\n",
        "      special = special + 1\n",
        "print(\"\\nTotal Number of Special Characters :  \", special)    #number of Special charcters\n",
        "print(specials)\n",
        "print(\"\\n\\nTotal Number of Numerics:  \", digits)      # Number of numerics\n",
        "print(digits_l)\n",
        "print(\"\\n\\nTotal Number of Upper case letters: \",len(upper_case))   #Number of upper case letters\n",
        "print(upper_case)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Total Number of Special Characters :   818\n",
            "['.', '.', '.', '.', ',', '.', '.', '(', ')', '[', ']', '-', ',', '.', '[', ']', '’', '.', ',', ',', '.', '.', ',', '.', '*', '*', '.', ',', ',', ',', ',', ',', ',', '&', '.', ',', ',', '-', '-', '-', ',', '.', ',', ',', ',', '.', '’', ',', '.', ',', '.', ',', ';', '*', '.', ',', '’', ',', ';', ',', '.', ',', ',', ',', '-', ',', ',', ',', '&', ',', ',', ',', '&', '.', ',', ',', '&', '.', ';', '.', ',', '(', ')', ',', ',', ',', ',', ',', ',', ',', ',', ',', '-', '.', ',', ',', ',', '.', ',', '.', ',', ',', ';', ',', ';', ',', ',', ',', ',', ',', ',', ',', ',', '.', '.', '.', ',', ',', ',', '.', '-', '-', '.', ',', ',', ',', ',', ',', ',', '.', '.', ',', ',', '[', '.', '.', ';', '.', '&', '.', '.', ';', '’', '.', ',', ';', '.', '.', ',', ';', '.', '.', ',', ']', ',', '[', '[', '[', '.', '.', '§', ',', '.', ',', ']', '*', ',', '.', ',', ',', ',', '.', '.', ',', ',', ',', '.', '[', '.', '.', ',', ';', '.', '.', ',', ';', '.', '.', ';', '.', '.', ';', '.', ',', ';', ']', ',', ',', ',', '’', '.', '’', '.', '.', '.', '.', '.', ',', '.', '.', ',', '-', '-', ',', '[', '.', '.', ',', ']', ',', ',', '.', ',', '.', '*', '*', ',', '.', ',', ',', ',', '.', '[', '[', '[', '[', '.', ',', '.', '.', ';', '.', '.', '.', '.', ';', '.', '.', '.', ']', ',', ',', ',', ',', '.', ',', '.', '.', ',', ',', ',', '.', ';', ',', ',', '.', ',', ',', '.', '[', '.', '-', ',', ';', '.', ',', '.', '.', ';', '.', ',', '.', '.', ';', '*', '.', ',', '.', '.', '.', '.', '&', ',', '.', ']', ',', '.', ',', ';', ',', '’', ',', ',', ',', ',', '.', ',', ',', ',', ',', ',', '.', ',', ',', '.', ',', ',', ',', ',', '.', ',', ',', '.', ',', ',', '-', '.', ',', ',', ',', ',', '.', ',', ',', ',', '.', ',', ',', '(', ',', ')', ',', ',', ',', '.', '*', ',', ',', '.', ',', ',', '.', '[', '.', ',', '’', '.', '.', ']', '*', '*', ',', ',', '.', ',', ',', ',', '&', '.', ',', ',', '.', '.', ',', ',', ',', '“', '’', ',', ',', '‘', ',', ';', ',', ';', ',', ',', '.', '”', '[', '.', ',', ',', '’', ',', '.', ';', '.', '&', '.', ';', ',', ',', '.', ']', '.', ',', ';', '.', ',', ',', '.', '.', ',', '“', ',', ',', '“', ',', ',', ',', '.', '”', '[', '.', '.', '.', ']', ',', ',', '.', ',', '?', ',', ',', '.', ',', ',', '*', '.', ',', ',', ',', ',', ',', '.', '.', ',', '&', '.', ',', ',', ',', '.', ',', '.', ',', '.', ',', ',', ',', ';', '.', ',', ',', ',', ',', ',', ',', '.', ',', '.', ',', ',', '.', ',', ',', ',', ':', '“', '.', '”', ',', '.', ',', ';', '.', '\\n', '*', '*', ',', ',', ',', ',', '.', ',', ',', ',', ',', ',', '.', ',', ',', ',', ',', '*', ',', ',', '.', ';', ',', ',', ',', '&', '.', ',', ';', ',', ',', '.', ',', ',', '.', ',', '.', '.', ',', '.', '*', '*', ',', '“', ',', ',', ',', '.', '”', '[', '’', '.', ',', '§', '.', ']', ',', ',', ',', ',', ',', '.', ',', ',', '.', '.', ',', ',', ',', ',', ',', ',', ',', '.', ',', ',', '“', '“', ',', '”', ',', ',', '.', ',', ':', ',', '*', '-', '-', '.', '*', '*', ',', ',', ',', '.', '“', '”', '.', ',', ',', ',', ',', ',', ',', ',', ',', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', '.', ',', ',', ',', ',', ',', '.', ',', ',', ';', ',', ',', ',', ',', '.', '.', ',', '©', '.', '.', '.', '.', '(', ')', '(', ')', '.', '.', '’', '’', '.', ',', ',', '.', ',', '.', '.', '.', '.', '.', '.', '—', '.', ',', '&', '.', '.', '.', ',', ',', '.', '[', '.', ']', '.', '.', '.', '.', '—', '.', '.', '.', ',', ',', '.', '[', ';', ';', '.', ']', '.', '.', '.', '.', '—', '.', '.', '.', ',', ',', '.', '[', '.', ']', '.', '.', '.', '.', '.', '—', '.', '.', '.', ',', '+', ',', '.', '.', '.', '.', '.', '.', '—', '.', '.', '.', ',', ',', '.', ',', '.', ',', ',', ':', ',', ',', '.', '.', '.', '—', '.', '.', '.', ',', ',', '.', '.', '.', '.', '.', '.', '.', '—', '.', '.', '.', '.', ',', ',', '.', ',', '.', '—', '—', '.', '.', '.', '.', ',', ',', ',', ',', ',', '.', '.', '.', '—', '—', '(', ')', '.', '.', '.', ',', '.', '.', '.', ',', ',', ',', '.', '.', '-', '.', ',', ',', '.', '.', '.', '.', '.', '.', ',', '.', ',', '.', '+', '.', '.', '.', ',', '.', '.', '.', ',', '.', '.', ',', '.', '“', ',', '.', ',', ',', ',', ',', '.', '.', '.', '.', '.', '.']\n",
            "\n",
            "\n",
            "Total Number of Numerics:   356\n",
            "['5', '7', '4', '0', '1', '8', '4', '3', '2', '1', '4', '2', '1', '8', '2', '1', '5', '1', '1', '8', '4', '0', '8', '0', '1', '0', '0', '1', '8', '3', '9', '7', '4', '1', '7', '1', '8', '4', '0', '1', '8', '4', '0', '1', '1', '8', '4', '0', '2', '1', '3', '6', '1', '1', '3', '0', '7', '6', '6', '0', '4', '1', '2', '4', '1', '8', '4', '2', '2', '7', '3', '4', '4', '1', '1', '6', '7', '7', '4', '2', '3', '1', '1', '2', '2', '0', '7', '3', '3', '3', '8', '4', '2', '4', '5', '2', '6', '1', '3', '2', '3', '5', '8', '6', '9', '3', '4', '1', '8', '2', '1', '1', '6', '7', '2', '2', '2', '1', '6', '3', '6', '6', '4', '1', '3', '0', '2', '9', '2', '2', '4', '1', '2', '3', '3', '2', '2', '4', '2', '2', '9', '1', '1', '2', '7', '4', '3', '9', '3', '9', '1', '8', '4', '0', '7', '4', '4', '5', '1', '8', '2', '3', '3', '6', '8', '1', '3', '9', '7', '6', '6', '0', '4', '1', '8', '2', '1', '1', '6', '7', '7', '4', '5', '4', '7', '4', '6', '4', '2', '1', '0', '4', '6', '7', '4', '7', '5', '5', '7', '4', '0', '1', '8', '4', '3', '2', '8', '4', '2', '0', '1', '9', '9', '1', '5', '5', '2', '6', '6', '2', '7', '1', '1', '8', '7', '6', '2', '4', '7', '3', '6', '2', '3', '7', '6', '1', '8', '7', '2', '3', '4', '5', '3', '2', '9', '3', '3', '4', '1', '8', '7', '1', '4', '3', '1', '5', '2', '6', '5', '2', '7', '1', '8', '5', '8', '5', '2', '1', '3', '3', '3', '3', '3', '5', '1', '8', '5', '2', '6', '8', '1', '4', '5', '1', '4', '7', '1', '8', '5', '7', '7', '6', '5', '2', '5', '6', '2', '5', '8', '1', '8', '8', '0', '8', '4', '9', '1', '3', '9', '1', '4', '1', '8', '8', '7', '9', '1', '0', '3', '4', '6', '4', '1', '9', '3', '6', '3', '1', '9', '3', '9', '1', '8', '2', '8', '2', '2', '5', '1', '8', '2', '1', '8', '3', '7', '2', '3', '9', '1', '0', '8', '1', '8', '1', '2', '2']\n",
            "\n",
            "\n",
            "Total Number of Upper case letters:  695\n",
            "['A', 'S', 'C', 'A', 'A', 'D', 'A', 'M', 'S', 'T', 'A', 'N', 'N', 'E', 'R', 'A', 'N', 'D', 'H', 'O', 'R', 'T', 'O', 'N', 'J', 'T', 'S', 'W', 'R', 'I', 'T', 'E', 'C', 'C', 'S', 'W', 'H', 'C', 'M', 'C', 'A', 'C', 'C', 'R', 'L', 'P', 'U', 'S', 'C', 'T', 'I', 'N', 'S', 'A', 'H', 'T', 'A', 'H', 'A', 'O', 'T', 'O', 'N', 'H', 'I', 'H', 'M', 'B', 'H', 'S', 'A', 'H', 'G', 'T', 'T', 'S', 'G', 'T', 'H', 'I', 'T', 'H', 'S', 'A', 'L', 'F', 'R', 'H', 'S', 'M', 'I', 'T', 'H', 'T', 'H', 'M', 'B', 'S', 'R', 'B', 'P', 'R', 'E', 'R', 'J', 'R', 'M', 'R', 'A', 'D', 'I', 'T', 'H', 'C', 'C', 'J', 'R', 'W', 'R', 'J', 'R', 'D', 'R', 'H', 'A', 'T', 'W', 'M', 'M', 'U', 'R', 'P', 'H', 'Y', 'W', 'G', 'J', 'O', 'N', 'E', 'S', 'A', 'D', 'T', 'T', 'I', 'W', 'F', 'J', 'R', 'W', 'C', 'C', 'R', 'H', 'R', 'I', 'O', 'C', 'O', 'L', 'L', 'I', 'E', 'R', 'C', 'J', 'T', 'T', 'C', 'C', 'C', 'W', 'F', 'J', 'R', 'S', 'D', 'J', 'R', 'A', 'S', 'C', 'R', 'S', 'R', 'L', 'A', 'T', 'I', 'T', 'W', 'I', 'A', 'M', 'T', 'T', 'C', 'T', 'T', 'H', 'P', 'E', 'M', 'P', 'R', 'T', 'I', 'M', 'D', 'T', 'A', 'B', 'A', 'W', 'H', 'W', 'F', 'P', 'S', 'B', 'P', 'E', 'B', 'W', 'F', 'T', 'B', 'T', 'T', 'I', 'A', 'D', 'N', 'I', 'I', 'T', 'T', 'T', 'M', 'H', 'T', 'P', 'B', 'U', 'S', 'W', 'G', 'T', 'I', 'T', 'I', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'I', 'D', 'I', 'S', 'S', 'E', 'N', 'T', 'I', 'N', 'G', 'O', 'P', 'I', 'N', 'I', 'O', 'N', 'O', 'R', 'M', 'O', 'N', 'D', 'J', 'T', 'C', 'D', 'I', 'I', 'I', 'T', 'T', 'S', 'I', 'T', 'I', 'I', 'T', 'N', 'T', 'I', 'T', 'I', 'A', 'C', 'A', 'W', 'L', 'E', 'D', 'T', 'R', 'N', 'U', 'S', 'G', 'W', 'C', 'R', 'T', 'T', 'D', 'T', 'D', 'H', 'C', 'B', 'J', 'A', 'A', 'A', 'T', 'C', 'C', 'C', 'C', 'A', 'P', 'P', 'E', 'A', 'L', 'C', 'C', 'H', 'T', 'H', 'M', 'J', 'S', 'A', 'F', 'F', 'O', 'L', 'D', 'D', 'T', 'C', 'C', 'L', 'D', 'C', 'M', 'A', 'A', 'T', 'R', 'O', 'V', 'E', 'R', 'F', 'O', 'R', 'C', 'O', 'N', 'V', 'E', 'R', 'S', 'I', 'O', 'N', 'O', 'F', 'C', 'O', 'T', 'T', 'O', 'N', 'A', 'P', 'P', 'E', 'A', 'L', 'C', 'C', 'M', 'T', 'H', 'J', 'O', 'H', 'N', 'D', 'C', 'U', 'N', 'N', 'I', 'N', 'G', 'H', 'A', 'M', 'J', 'T', 'C', 'C', 'B', 'J', 'A', 'A', 'G', 'A', 'R', 'N', 'I', 'S', 'H', 'M', 'E', 'N', 'T', 'W', 'A', 'G', 'E', 'S', 'W', 'A', 'I', 'V', 'E', 'R', 'O', 'F', 'E', 'X', 'E', 'M', 'P', 'T', 'I', 'O', 'N', 'A', 'P', 'P', 'E', 'A', 'L', 'C', 'C', 'M', 'T', 'H', 'J', 'O', 'H', 'N', 'D', 'C', 'U', 'N', 'N', 'I', 'N', 'G', 'H', 'A', 'M', 'J', 'T', 'C', 'C', 'M', 'K', 'L', 'A', 'A', 'T', 'R', 'I', 'A', 'L', 'O', 'F', 'R', 'I', 'G', 'H', 'T', 'O', 'F', 'P', 'R', 'O', 'P', 'E', 'R', 'T', 'Y', 'I', 'N', 'C', 'O', 'T', 'T', 'O', 'N', 'A', 'P', 'P', 'E', 'A', 'L', 'C', 'C', 'B', 'T', 'H', 'S', 'D', 'H', 'A', 'L', 'E', 'J', 'T', 'C', 'C', 'E', 'L', 'A', 'A', 'E', 'R', 'R', 'O', 'R', 'C', 'C', 'A', 'T', 'H', 'A', 'B', 'M', 'O', 'O', 'R', 'E', 'J', 'T', 'C', 'C', 'D', 'B', 'C', 'C', 'T', 'C', 'J', 'S', 'C', 'B', 'C', 'C', 'J', 'T', 'C', 'M', 'R', 'C', 'A', 'A', 'T', 'C', 'T', 'B', 'C', 'A', 'P', 'P', 'E', 'A', 'L', 'C', 'C', 'E', 'T', 'H', 'W', 'M', 'L', 'W', 'H', 'I', 'T', 'L', 'O', 'C', 'K', 'N', 'T', 'C', 'M', 'E', 'T', 'S', 'W', 'T', 'A', 'W', 'M', 'C', 'G', 'A', 'L', 'R', 'G', 'A', 'L', 'R', 'T', 'A', 'T', 'R', 'T', 'T', 'D', 'Q', 'P', 'N', 'M', 'A', 'S', 'C', 'N', 'Y', 'S', 'P', 'A', 'A', 'W', 'C', 'C', 'P', 'M', 'P', 'A', 'O', 'C', 'C', 'T', 'C', 'M', 'S', 'D', 'J', 'N', 'Y', 'S', 'A', 'B', 'A', 'A', 'C', 'F', 'T', 'F', 'N', 'T', 'T', 'N', 'T', 'H', 'T', 'H']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufRe5kFrVfa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1.2\n",
        "lower_case=[]\n",
        "for i in words:\n",
        "  lower_case.append(i.lower())      #converted to lower case letters\n",
        "uncleaned_df=pd.DataFrame(lower_case,columns=['Data'])    #removed all the punctuation\n",
        "df= uncleaned_df['Data'].apply(lambda x:''.join([i for i in x if i not in string.punctuation])).to_frame()\n",
        "df2= df['Data'].map(lambda x: re.sub(r'\\W+', ' ', x))\n",
        "cleaned_data=df2.values.tolist()\n",
        "cleaned_data\n",
        "sent=[]\n",
        "for i in cleaned_data:\n",
        "  stop_words = set(stopwords.words('english'))        #Removed all the stop words\n",
        "  word_tokens = word_tokenize(i) \n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "  filtered_sentence = [] \n",
        "  for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        filtered_sentence.append(w) \n",
        "  #print()\n",
        "  sent.append(' '.join(word for word in filtered_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg3LDCmTiYWf",
        "colab_type": "code",
        "outputId": "f8aee44d-7206-4582-f1a6-6430a5d1ffd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#1.2\n",
        "import itertools\n",
        "from collections import Counter \n",
        "freq=[]\n",
        "strings_all=''\n",
        "for j in sent:                            #Retrieved Top 5 most frequent words and Top 5 rare words\n",
        "  freq.append(j.split())\n",
        "  strings_all+=' '+j\n",
        "a=list(itertools.chain.from_iterable(freq))\n",
        "def getDuplicatesWithCount(listOfElems):\n",
        "    dictOfElems = dict()\n",
        "    for elem in listOfElems:\n",
        "        if elem in dictOfElems:\n",
        "            dictOfElems[elem] += 1\n",
        "        else:\n",
        "            dictOfElems[elem] = 1    \n",
        "    dictOfElems = { key:value for key, value in dictOfElems.items() if value > 1}\n",
        "    return dictOfElems\n",
        "dictOfElems = getDuplicatesWithCount(a) \n",
        "d=Counter(dictOfElems)\n",
        "top=[]\n",
        "least=[]\n",
        "for k, v in d.most_common(5):\n",
        "  top.append('%s'%(k,))\n",
        "x = strings_all.split()\n",
        "count_l=0\n",
        "for y in x:\n",
        "    if strings_all.count(y) == 1:\n",
        "      least.append(y)\n",
        "      count_l=count_l+1\n",
        "      if count_l==5:\n",
        "        break\n",
        "    else:\n",
        "        pass\n",
        "top_least=top+least\n",
        "print('Most repeated words-->',d.most_common(5))\n",
        "print('Rare words -->',least)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most repeated words--> [('execution', 50), ('crop', 49), ('lien', 25), ('levy', 25), ('v', 22)]\n",
            "Rare words --> ['supreme', 'alabama', 'adams', 'tanner', 'horton']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDdCWZyy6GzJ",
        "colab_type": "code",
        "outputId": "c0196e93-f419-4c0b-b8cb-261289730898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "pip install pyspellchecker"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/d1/ec4e830e9f9c1fd788e1459dd09279fdf807bc7a475579fd7192450b879c/pyspellchecker-0.5.4-py2.py3-none-any.whl (1.9MB)\n",
            "\r\u001b[K     |▏                               | 10kB 15.2MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |▊                               | 40kB 2.8MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 3.2MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 3.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 81kB 4.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 92kB 4.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 102kB 4.6MB/s eta 0:00:01\r\u001b[K     |█▉                              | 112kB 4.6MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 4.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 133kB 4.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 143kB 4.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 153kB 4.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 163kB 4.6MB/s eta 0:00:01\r\u001b[K     |███                             | 174kB 4.6MB/s eta 0:00:01\r\u001b[K     |███                             | 184kB 4.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 194kB 4.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 204kB 4.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 215kB 4.6MB/s eta 0:00:01\r\u001b[K     |███▊                            | 225kB 4.6MB/s eta 0:00:01\r\u001b[K     |████                            | 235kB 4.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 245kB 4.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 256kB 4.6MB/s eta 0:00:01\r\u001b[K     |████▌                           | 266kB 4.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 276kB 4.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 286kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 296kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 307kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 317kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 327kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 337kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 348kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 358kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 368kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 378kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 389kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 399kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 409kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 419kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 430kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 440kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 450kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 460kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 471kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 481kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 491kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 501kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 512kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 522kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 532kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 542kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 552kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 563kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 573kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 583kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 593kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 604kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 614kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 624kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 634kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 645kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 655kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 665kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 675kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 686kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 696kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 706kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 716kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 727kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 737kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 747kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 757kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 768kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 778kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 788kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 798kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 808kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 819kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 829kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 839kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 849kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 860kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 870kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 880kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 890kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 901kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 911kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 921kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 931kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 942kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 952kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 962kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 972kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 983kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 993kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.0MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.0MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.0MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.0MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.0MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.1MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.1MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.1MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.1MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.1MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.1MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.1MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.1MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.1MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.2MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.2MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.2MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.2MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.2MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.2MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.2MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 1.2MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.2MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.3MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.3MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.3MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.3MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.3MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.3MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.3MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.3MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.3MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.4MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.4MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.4MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.4MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.4MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.4MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.4MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.4MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.4MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.4MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.5MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.5MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.5MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.5MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.5MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.5MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.5MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.5MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.5MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.5MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.6MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.6MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.6MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.6MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.6MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.6MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.6MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.6MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.6MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.6MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.7MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.7MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.7MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.7MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.7MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.7MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.7MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.7MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.7MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.8MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.8MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.8MB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.8MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.8MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.8MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.8MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.8MB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.8MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.8MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.9MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.9MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.9MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.9MB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.9MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.9MB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.9MB 4.6MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn6HTBnszc8W",
        "colab_type": "code",
        "outputId": "4373a353-7581-4d6b-d130-ae68ec9ef6bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#1.2\n",
        "from spellchecker import SpellChecker \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "spell = SpellChecker() \n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "rep_rem=[]\n",
        "for a in sent:                                      #Removed all the top 5 Repeated words and top 5 rare words\n",
        "  word_tokens = word_tokenize(a)                    #Tokenization\n",
        "  top_least_removed = [w for w in word_tokens if not w in top_least] \n",
        "  top_least_removed = [] \n",
        "  for w in word_tokens: \n",
        "    if w not in top_least: \n",
        "      spell_check=spell.correction(w)   #spelling check\n",
        "      stemmed=ps.stem(spell_check)        #stemming\n",
        "      lemmatized=lemmatizer.lemmatize(stemmed)   #Lemmatization     \n",
        "      top_least_removed.append(lemmatized)            \n",
        "  rep_rem.append(' '.join(word for word in top_least_removed))   \n",
        "while(\"\" in rep_rem) : \n",
        "    rep_rem.remove(\"\") \n",
        "rep_rem"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ala',\n",
              " 'court',\n",
              " 'june term',\n",
              " 'synopsi',\n",
              " 'writ error circuit court summer',\n",
              " 'west headnot',\n",
              " 'chattel mortgag',\n",
              " 'crop',\n",
              " 'grow exist subject-matt mortgag contract pas interest vest posse either immedi futur time',\n",
              " 'case cite headnot',\n",
              " 'creditor remedi',\n",
              " 'prioriti',\n",
              " 'st prohibit gather attach favor fi',\n",
              " 'fa',\n",
              " 'grow attach gather',\n",
              " 'case cite headnot',\n",
              " 'trial right properti statut',\n",
              " 'novemb issu circuit court summer suit plaintiff error requir sheriff counti make good c',\n",
              " 'allen harrison other sum thirty-seven hundr seventy-seven dollar besid cost',\n",
              " 'levi thirti bale cotton properti allen harrison claim bond given tri right',\n",
              " 'issu made tri question liabil cotton plaintiff submit juri',\n",
              " 'trial bill except seal instanc plaintiff',\n",
              " 'plaintiff prove recov judgment octob issu thereon th nov thereaft alia juri fieri facial issu regularli time made cotton levi growl plantat harrison cultiv hand servic',\n",
              " 'prove claimant product written contract harrison twenty-second may consider claimant involv inform burton harrison summer counti expo amount upward fourteen thousand dollar bargain sold claimant grow cotton c',\n",
              " 'consist one hundr twenti acr c',\n",
              " 'allen harrison promis oblig give use claimant time save suffer inform matur gather undertook deliv cotton janesvil',\n",
              " 'claimant came tennesse resid first septemb bring three four white labour took posse slave latter white labour gather cotton prepar market levi warehous janesvil',\n",
              " 'plaintiff prove harrison claimant took posse absent dispos without consent',\n",
              " 'admit contract made good faith',\n",
              " 'court charg juri plaintiff virtu judgment grow harrison right convey without manner restrain write adduc sale fieri facial would attach upon gather yet claimant obtain posse first septemb control gather attach subject',\n",
              " 'attorney law firm',\n",
              " 'r h smith plaintiff error made follow point',\n",
              " 'harrison must may immatur state insist subject sale',\n",
              " 'common law grow could levi sold talk rep bo p rep east rep note john rep mass rep statu ask dig p forbid grow receiv strict construct',\n",
              " 'mere inhibit attach sale may made matur gather',\n",
              " 'contract purport convey grow mere executori agreement requir act done harrison order invest claimant right properti',\n",
              " 'chit con john rep wend rep john rep down rep act done matter gather liabl seiz harrison debt',\n",
              " 'court chanceri would compel specif perform contract claimant instanc',\n",
              " 'charg court also objection decid disput fact',\n",
              " 'w murphi w g jone defendantcit act ask dig declar law plant gather contend attach favor plaintiff',\n",
              " 'case defend restrain make contract claimant',\n",
              " 'destroy injunct take away right',\n",
              " 'short whenev right temporarili suspend withdrawn time lost',\n",
              " 'whippl foot john rep wash c c rep rep',\n",
              " 'admit contract defend claimant good faith sever remov cotton gave latter good titl creditor former',\n",
              " 'opinion',\n",
              " 'collier c j',\n",
              " 'doubt grow exist subject matter sale mortgag contract posse interest vest posse either immedi futur time',\n",
              " 'proposit frequent assum unquestion point inquiri gener whether statut fraud cha',\n",
              " 'mere chattel transfer parol without write',\n",
              " 'chitti con whippl foot john rep stewart doughti john rep austin sawyer cow rep see also rames lee alston last term contract set bill except inclin think evid rather mortgag absolut sale',\n",
              " 'recit claimant involv inform mercantil firm defend partner upward fourteen thousand dollar estat sheriff hand convey made cotton corn oat grantor agre give time use claimant prevent injuri inform',\n",
              " 'defend might time divest interest contract vest claimant discharg liabil inform judgment creditor might satisfi gather levi sold fieri facial',\n",
              " 'consid write claimant assert right mortgag power take posse time year unless reliev engag inform',\n",
              " 'pretend liabil satisfi admit parti act good faith dri question law whether right plaintiff claimant shall prevail',\n",
              " 'assum present plaintiff oper upon plant previou contract may inquir whether defend interest could levi sold',\n",
              " 'claimant previou taken posse prepar cotton market remov warehous',\n",
              " 'posse insist trespass acquir absenc defend without consent given',\n",
              " 'conced truth fact state bill except think follow posse claimant nulliti case must consid never interf',\n",
              " 'contract contain express undertak give time claimant might requir indemn took posse absenc grantor though without consent subsequ acquiesc infer would necessari act approv',\n",
              " 'take clear law seen defend time noth mere equit right redeem cotton pay debt endors claimant',\n",
              " 'posse coupl equiti nake equiti held reach ordinari',\n",
              " 'perkin elliott mayfield porter rep',\n",
              " 'bring u back question whether plaintiff grow defeat mortgag claimant',\n",
              " 'frequent moot whether common law corn c',\n",
              " 'gather seiz fieri facial',\n",
              " 'mr dane remark upon point say american editor bacon abridg say wheat grow ground chattel subject taken sheriff may suffer grow till harvest cut sell may perhap sell grow purchas entitl enter purpos cut carri away cite whippl foot ut supra also pool case talk bo p east n whippl foot seem case support posit unrip wheat corn may taken editor state noth taken sold posit say learn comment doubt law unnecessari consid matter stand common law first section act prevent sheriff offic levi execut certain case enact shall law sheriff offic writ fieri facial plant debtor person may issu gather ask dig express inhibit remain ground sever soil owe growth respect properti thu situat attach eo instant upon place hand offic act cite effect keep right abey gather oper prevent debtor dispos properti attach give creditor right sold satisfi judgment right intim connect latter taken away suspend effect common law destruct former principl fulli establish manson harwel presid c bank unit state assigne citat contain opinion court case also opinion wood gari et al decid last term compet legislatur made unlaw particular properti condit chang still give continu doubt noth act question indic intent object mere suspend sale gather would easi said explicit term declar statut totem verb shall levi legislatur must suppos meant express act induc doubt exist common law intend remov doubt declar law futur creat author case law exist silent idea attach upon plant soon deliv sheriff though right postpon sever took place attempt deduc last word section cite viz gather',\n",
              " 'word upon principl construct regard potent give retrospect effect refer would postpon gather relat postpon event take place',\n",
              " 'right plant expressli taken away statut connect consequ upon right never attach sever case right defend make contract unquestion titl claimant coupl posse paramount could exert',\n",
              " 'circuit judg may mistaken law suppos contract sale error respect immateri whether sale mortgag seen fact case defend interest could seiz sold assumpt materi fact charg posse claimant time acquir gather c refer determin juri instruct find accord evid adduc ever attach favor plaintiff bona fide contract conced charg necessari point could proprieti enter inquiri juri',\n",
              " 'result said judgment circuit court affirm',\n",
              " 'dissent opinion',\n",
              " 'ormond j',\n",
              " 'statut present question court shall law sheriff offic writ fidei facial plant debtor person may issu gather clay dig',\n",
              " 'shall enter upon enquiri whether common law could levi upon grow though apprehend would difficult maintain affirm proposit',\n",
              " 'suffici purpos statut suppos law doubtless practic',\n",
              " 'act must consid connect act upon subject',\n",
              " 'polici state indic statut undeni properti debtor real person legal titl shall subject sale appear would difficult assign reason exempt speci properti claim judgment creditor give defend right dispos',\n",
              " 'appear defer argument sheriff prohibit levi plant therefor lost debtor right sell non loquitur',\n",
              " 'mischief statut design remedi sacrific would necessarili made sale immatur statut enabl debtor retain matur sever soil put condit bring valueth mean time continu plaintiff',\n",
              " 'confirm correct view necessari found think languag employ legislatur',\n",
              " 'sheriff forbidden plant gather',\n",
              " 'view taken major court correct right secur plaintiff levi gather may frustrat case sale defend whilst immatur state',\n",
              " 'construct put upon statut involv singular anomali legislatur protect debtor forbidden plaintiff sell properti debtor condit bring valu yet permit debtor voluntarili sale submit sacrific benefit',\n",
              " 'effect gift defend grow provid gather dispos condit',\n",
              " 'feel thorough convict intent legislatur secur loss prohibit sale gather temporari suspens right sell ceas',\n",
              " 'citat',\n",
              " 'ala we',\n",
              " 'end document',\n",
              " 'thomson reuter',\n",
              " 'claim origin u govern work',\n",
              " 'cite refer',\n",
              " 'treatment',\n",
              " 'titl',\n",
              " 'date',\n",
              " 'type',\n",
              " 'depth',\n",
              " 'headnot',\n",
              " 'cite',\n",
              " 'booker jone am x',\n",
              " 'ala ala',\n",
              " 'trover convers cotton count case',\n",
              " 'appeal circuit court hale',\n",
              " 'tri hon',\n",
              " 'j scaffold',\n",
              " 'dec term',\n",
              " 'case',\n",
              " 'cite',\n",
              " 'lehman burr co marshal',\n",
              " 'ala ala',\n",
              " 'trover convers cotton appeal citi court montgomeri',\n",
              " 'tri hon',\n",
              " 'john cunningham',\n",
              " 'jan term',\n",
              " 'case',\n",
              " 'cite',\n",
              " 'bib jancey',\n",
              " 'ala ala',\n",
              " 'banish wage waiver exempt appeal citi court montgomeri',\n",
              " 'tri hon',\n",
              " 'john cunningham',\n",
              " 'jan term',\n",
              " 'case',\n",
              " 'cite',\n",
              " 'mckenzi lamprey',\n",
              " 'ala ala',\n",
              " 'trial right properti cotton appeal circuit court barbour',\n",
              " 'tri hon',\n",
              " 'hale',\n",
              " 'jan term',\n",
              " 'case',\n",
              " 'cite',\n",
              " 'evan lamar',\n",
              " 'ala ala',\n",
              " 'error circuit court autauga',\n",
              " 'tri hon',\n",
              " 'b moor',\n",
              " 'jun term',\n",
              " 'case',\n",
              " 'cite',\n",
              " 'dewey bowman',\n",
              " 'cal cal',\n",
              " 'judgment court jacob cohen revers follow reason find court far cohen concern',\n",
              " 'jul term',\n",
              " 'case',\n",
              " 'mention',\n",
              " 'ree coat',\n",
              " 'ala ala',\n",
              " 'trover convers three bale cotton',\n",
              " 'appeal circuit court ecowa',\n",
              " 'tri hon',\n",
              " 'wm',\n",
              " 'l whitlock',\n",
              " 'nov term',\n",
              " 'case',\n",
              " 'mention',\n",
              " 'edward thompson',\n",
              " 'sw ten',\n",
              " 'appeal circuit court weakli counti',\n",
              " 'may',\n",
              " 'case',\n",
              " 'grow crop subject seizur attach',\n",
              " 'all',\n",
              " 'gener common law grow crop rais annual plant still attach soil regard person chattel subject seizur attach',\n",
              " 'all',\n",
              " 'tabl author',\n",
              " 'treatment',\n",
              " 'referenc titl',\n",
              " 'type',\n",
              " 'depth',\n",
              " 'quot',\n",
              " 'page number',\n",
              " 'mention',\n",
              " 'austin sawyer',\n",
              " 'cow',\n",
              " 'syrup',\n",
              " 'parol evid admiss contradict substanti vari written contract',\n",
              " 'quitclaim land w wheat grow reserv',\n",
              " 'case',\n",
              " 'cite',\n",
              " 'perkin mayfield',\n",
              " 'port',\n",
              " 'ala',\n",
              " 'writ error circuit court tuskaloosa',\n",
              " 'case',\n",
              " 'mention',\n",
              " 'stewart doughti',\n",
              " 'john',\n",
              " 'syrup',\n",
              " 'let b farm six year agre render yield pay one half wheat rye corn grain rais farm year',\n",
              " 'case',\n",
              " 'file',\n",
              " 'file citat',\n",
              " 'neg treatment',\n",
              " 'neg treatment result citat',\n",
              " 'histori',\n",
              " 'histori result citat']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gFUHknM6s9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1.3\n",
        "pd.DataFrame(rep_rem,columns=['cleaned']).to_csv('final.csv') #saved to csv file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH2lTKIw0NvW",
        "colab_type": "code",
        "outputId": "39ff98f4-469e-4134-b795-a621d4e89bde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#1.4\n",
        "def freq(str): \n",
        "\tstr_list = str.split() \n",
        "\tunique_words = set(str_list) \t    #Frequency of all the terms\n",
        "\tfor o in unique_words : \n",
        "\t\tprint('Frequency Of-->', o , '->', str_list.count(o)) \n",
        "st=''\n",
        "for k in rep_rem:\n",
        "  st+=' '+k\n",
        "freq(st) "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frequency Of--> cunningham -> 2\n",
            "Frequency Of--> came -> 1\n",
            "Frequency Of--> polici -> 1\n",
            "Frequency Of--> sold -> 7\n",
            "Frequency Of--> retain -> 1\n",
            "Frequency Of--> immedi -> 2\n",
            "Frequency Of--> would -> 8\n",
            "Frequency Of--> point -> 4\n",
            "Frequency Of--> bib -> 1\n",
            "Frequency Of--> defend -> 12\n",
            "Frequency Of--> plaintiff -> 15\n",
            "Frequency Of--> cultiv -> 1\n",
            "Frequency Of--> last -> 3\n",
            "Frequency Of--> appeal -> 6\n",
            "Frequency Of--> evid -> 3\n",
            "Frequency Of--> promis -> 1\n",
            "Frequency Of--> moor -> 1\n",
            "Frequency Of--> citi -> 2\n",
            "Frequency Of--> harwel -> 1\n",
            "Frequency Of--> jone -> 2\n",
            "Frequency Of--> refer -> 3\n",
            "Frequency Of--> besid -> 1\n",
            "Frequency Of--> hale -> 2\n",
            "Frequency Of--> remark -> 1\n",
            "Frequency Of--> acquir -> 2\n",
            "Frequency Of--> ut -> 1\n",
            "Frequency Of--> perkin -> 2\n",
            "Frequency Of--> think -> 3\n",
            "Frequency Of--> lamar -> 1\n",
            "Frequency Of--> enquiri -> 1\n",
            "Frequency Of--> done -> 2\n",
            "Frequency Of--> waiver -> 1\n",
            "Frequency Of--> meant -> 1\n",
            "Frequency Of--> idea -> 1\n",
            "Frequency Of--> inquir -> 1\n",
            "Frequency Of--> convict -> 1\n",
            "Frequency Of--> bring -> 4\n",
            "Frequency Of--> labour -> 2\n",
            "Frequency Of--> temporarili -> 1\n",
            "Frequency Of--> attorney -> 1\n",
            "Frequency Of--> hundr -> 2\n",
            "Frequency Of--> abridg -> 1\n",
            "Frequency Of--> admit -> 3\n",
            "Frequency Of--> conced -> 2\n",
            "Frequency Of--> paramount -> 1\n",
            "Frequency Of--> defeat -> 1\n",
            "Frequency Of--> wheat -> 4\n",
            "Frequency Of--> valu -> 1\n",
            "Frequency Of--> fi -> 1\n",
            "Frequency Of--> unnecessari -> 1\n",
            "Frequency Of--> rye -> 1\n",
            "Frequency Of--> mckenzi -> 1\n",
            "Frequency Of--> rames -> 1\n",
            "Frequency Of--> frustrat -> 1\n",
            "Frequency Of--> exempt -> 2\n",
            "Frequency Of--> could -> 6\n",
            "Frequency Of--> obtain -> 1\n",
            "Frequency Of--> redeem -> 1\n",
            "Frequency Of--> easi -> 1\n",
            "Frequency Of--> regard -> 2\n",
            "Frequency Of--> carri -> 1\n",
            "Frequency Of--> file -> 2\n",
            "Frequency Of--> headnot -> 4\n",
            "Frequency Of--> difficult -> 2\n",
            "Frequency Of--> render -> 1\n",
            "Frequency Of--> prepar -> 2\n",
            "Frequency Of--> bowman -> 1\n",
            "Frequency Of--> mere -> 5\n",
            "Frequency Of--> clay -> 1\n",
            "Frequency Of--> judgment -> 7\n",
            "Frequency Of--> ecowa -> 1\n",
            "Frequency Of--> inclin -> 1\n",
            "Frequency Of--> claim -> 3\n",
            "Frequency Of--> assumpt -> 1\n",
            "Frequency Of--> counti -> 3\n",
            "Frequency Of--> restrain -> 2\n",
            "Frequency Of--> remov -> 3\n",
            "Frequency Of--> agre -> 2\n",
            "Frequency Of--> bargain -> 1\n",
            "Frequency Of--> neg -> 2\n",
            "Frequency Of--> therefor -> 1\n",
            "Frequency Of--> whilst -> 1\n",
            "Frequency Of--> intim -> 1\n",
            "Frequency Of--> defendantcit -> 1\n",
            "Frequency Of--> matur -> 3\n",
            "Frequency Of--> mistaken -> 1\n",
            "Frequency Of--> suppos -> 3\n",
            "Frequency Of--> other -> 1\n",
            "Frequency Of--> indic -> 2\n",
            "Frequency Of--> creditor -> 5\n",
            "Frequency Of--> admiss -> 1\n",
            "Frequency Of--> ormond -> 1\n",
            "Frequency Of--> without -> 5\n",
            "Frequency Of--> non -> 1\n",
            "Frequency Of--> taken -> 7\n",
            "Frequency Of--> right -> 21\n",
            "Frequency Of--> edward -> 1\n",
            "Frequency Of--> fieri -> 5\n",
            "Frequency Of--> continu -> 2\n",
            "Frequency Of--> thompson -> 1\n",
            "Frequency Of--> suit -> 1\n",
            "Frequency Of--> liabl -> 1\n",
            "Frequency Of--> specif -> 1\n",
            "Frequency Of--> synopsi -> 1\n",
            "Frequency Of--> u -> 2\n",
            "Frequency Of--> necessarili -> 1\n",
            "Frequency Of--> mortgag -> 7\n",
            "Frequency Of--> let -> 1\n",
            "Frequency Of--> instanc -> 2\n",
            "Frequency Of--> sever -> 5\n",
            "Frequency Of--> moot -> 1\n",
            "Frequency Of--> growth -> 1\n",
            "Frequency Of--> judg -> 1\n",
            "Frequency Of--> practic -> 1\n",
            "Frequency Of--> correct -> 2\n",
            "Frequency Of--> parol -> 2\n",
            "Frequency Of--> purport -> 1\n",
            "Frequency Of--> law -> 18\n",
            "Frequency Of--> writ -> 4\n",
            "Frequency Of--> novemb -> 1\n",
            "Frequency Of--> c -> 9\n",
            "Frequency Of--> marshal -> 1\n",
            "Frequency Of--> connect -> 3\n",
            "Frequency Of--> make -> 3\n",
            "Frequency Of--> real -> 1\n",
            "Frequency Of--> enact -> 1\n",
            "Frequency Of--> histori -> 2\n",
            "Frequency Of--> remedi -> 2\n",
            "Frequency Of--> resid -> 1\n",
            "Frequency Of--> servic -> 1\n",
            "Frequency Of--> one -> 2\n",
            "Frequency Of--> took -> 4\n",
            "Frequency Of--> still -> 2\n",
            "Frequency Of--> lamprey -> 1\n",
            "Frequency Of--> burr -> 1\n",
            "Frequency Of--> compel -> 1\n",
            "Frequency Of--> thereon -> 1\n",
            "Frequency Of--> also -> 4\n",
            "Frequency Of--> put -> 2\n",
            "Frequency Of--> undertook -> 1\n",
            "Frequency Of--> expressli -> 1\n",
            "Frequency Of--> pool -> 1\n",
            "Frequency Of--> fide -> 1\n",
            "Frequency Of--> barbour -> 1\n",
            "Frequency Of--> determin -> 1\n",
            "Frequency Of--> seem -> 1\n",
            "Frequency Of--> found -> 1\n",
            "Frequency Of--> dissent -> 1\n",
            "Frequency Of--> jacob -> 1\n",
            "Frequency Of--> count -> 1\n",
            "Frequency Of--> rais -> 2\n",
            "Frequency Of--> harvest -> 1\n",
            "Frequency Of--> unlaw -> 1\n",
            "Frequency Of--> legal -> 1\n",
            "Frequency Of--> firm -> 2\n",
            "Frequency Of--> contain -> 2\n",
            "Frequency Of--> affirm -> 2\n",
            "Frequency Of--> bill -> 3\n",
            "Frequency Of--> opinion -> 4\n",
            "Frequency Of--> chanceri -> 1\n",
            "Frequency Of--> induc -> 1\n",
            "Frequency Of--> effect -> 4\n",
            "Frequency Of--> written -> 2\n",
            "Frequency Of--> silent -> 1\n",
            "Frequency Of--> fraud -> 1\n",
            "Frequency Of--> equit -> 1\n",
            "Frequency Of--> said -> 2\n",
            "Frequency Of--> tabl -> 1\n",
            "Frequency Of--> mischief -> 1\n",
            "Frequency Of--> clear -> 1\n",
            "Frequency Of--> establish -> 1\n",
            "Frequency Of--> market -> 2\n",
            "Frequency Of--> contradict -> 1\n",
            "Frequency Of--> st -> 1\n",
            "Frequency Of--> wage -> 1\n",
            "Frequency Of--> enabl -> 1\n",
            "Frequency Of--> gari -> 1\n",
            "Frequency Of--> state -> 6\n",
            "Frequency Of--> coupl -> 2\n",
            "Frequency Of--> find -> 2\n",
            "Frequency Of--> thomson -> 1\n",
            "Frequency Of--> unit -> 1\n",
            "Frequency Of--> scaffold -> 1\n",
            "Frequency Of--> lehman -> 1\n",
            "Frequency Of--> postpon -> 3\n",
            "Frequency Of--> respect -> 2\n",
            "Frequency Of--> suspens -> 1\n",
            "Frequency Of--> consider -> 1\n",
            "Frequency Of--> question -> 5\n",
            "Frequency Of--> revers -> 1\n",
            "Frequency Of--> work -> 1\n",
            "Frequency Of--> absolut -> 1\n",
            "Frequency Of--> quot -> 1\n",
            "Frequency Of--> posit -> 2\n",
            "Frequency Of--> bo -> 2\n",
            "Frequency Of--> attach -> 14\n",
            "Frequency Of--> benefit -> 1\n",
            "Frequency Of--> requir -> 3\n",
            "Frequency Of--> entitl -> 1\n",
            "Frequency Of--> made -> 8\n",
            "Frequency Of--> executori -> 1\n",
            "Frequency Of--> keep -> 1\n",
            "Frequency Of--> octob -> 1\n",
            "Frequency Of--> though -> 3\n",
            "Frequency Of--> till -> 1\n",
            "Frequency Of--> b -> 2\n",
            "Frequency Of--> reach -> 1\n",
            "Frequency Of--> dewey -> 1\n",
            "Frequency Of--> support -> 1\n",
            "Frequency Of--> attempt -> 1\n",
            "Frequency Of--> seal -> 1\n",
            "Frequency Of--> slave -> 1\n",
            "Frequency Of--> harrison -> 11\n",
            "Frequency Of--> stewart -> 2\n",
            "Frequency Of--> either -> 2\n",
            "Frequency Of--> evan -> 1\n",
            "Frequency Of--> sale -> 13\n",
            "Frequency Of--> assert -> 1\n",
            "Frequency Of--> wm -> 1\n",
            "Frequency Of--> date -> 1\n",
            "Frequency Of--> instruct -> 1\n",
            "Frequency Of--> mayfield -> 2\n",
            "Frequency Of--> grantor -> 2\n",
            "Frequency Of--> loquitur -> 1\n",
            "Frequency Of--> relat -> 1\n",
            "Frequency Of--> wood -> 1\n",
            "Frequency Of--> divest -> 1\n",
            "Frequency Of--> booker -> 1\n",
            "Frequency Of--> treatment -> 4\n",
            "Frequency Of--> ala -> 15\n",
            "Frequency Of--> error -> 6\n",
            "Frequency Of--> express -> 3\n",
            "Frequency Of--> facial -> 6\n",
            "Frequency Of--> page -> 1\n",
            "Frequency Of--> oblig -> 1\n",
            "Frequency Of--> remain -> 1\n",
            "Frequency Of--> held -> 1\n",
            "Frequency Of--> thirty-seven -> 1\n",
            "Frequency Of--> permit -> 1\n",
            "Frequency Of--> favor -> 3\n",
            "Frequency Of--> section -> 2\n",
            "Frequency Of--> materi -> 1\n",
            "Frequency Of--> cal -> 2\n",
            "Frequency Of--> gift -> 1\n",
            "Frequency Of--> certain -> 1\n",
            "Frequency Of--> except -> 3\n",
            "Frequency Of--> fulli -> 1\n",
            "Frequency Of--> issu -> 6\n",
            "Frequency Of--> offic -> 4\n",
            "Frequency Of--> mr -> 1\n",
            "Frequency Of--> three -> 2\n",
            "Frequency Of--> west -> 1\n",
            "Frequency Of--> assign -> 1\n",
            "Frequency Of--> event -> 1\n",
            "Frequency Of--> annual -> 1\n",
            "Frequency Of--> consent -> 3\n",
            "Frequency Of--> legislatur -> 5\n",
            "Frequency Of--> objection -> 1\n",
            "Frequency Of--> mean -> 1\n",
            "Frequency Of--> argument -> 1\n",
            "Frequency Of--> regularli -> 1\n",
            "Frequency Of--> common -> 7\n",
            "Frequency Of--> doughti -> 2\n",
            "Frequency Of--> chang -> 1\n",
            "Frequency Of--> never -> 2\n",
            "Frequency Of--> may -> 12\n",
            "Frequency Of--> posse -> 14\n",
            "Frequency Of--> contend -> 1\n",
            "Frequency Of--> bale -> 2\n",
            "Frequency Of--> cut -> 2\n",
            "Frequency Of--> deduc -> 1\n",
            "Frequency Of--> satisfi -> 3\n",
            "Frequency Of--> ceas -> 1\n",
            "Frequency Of--> recit -> 1\n",
            "Frequency Of--> end -> 1\n",
            "Frequency Of--> injunct -> 1\n",
            "Frequency Of--> instant -> 1\n",
            "Frequency Of--> subject -> 8\n",
            "Frequency Of--> oper -> 2\n",
            "Frequency Of--> porter -> 1\n",
            "Frequency Of--> shall -> 6\n",
            "Frequency Of--> purpos -> 2\n",
            "Frequency Of--> inform -> 6\n",
            "Frequency Of--> seen -> 2\n",
            "Frequency Of--> place -> 3\n",
            "Frequency Of--> th -> 1\n",
            "Frequency Of--> thousand -> 2\n",
            "Frequency Of--> liabil -> 3\n",
            "Frequency Of--> statut -> 10\n",
            "Frequency Of--> sheriff -> 9\n",
            "Frequency Of--> h -> 1\n",
            "Frequency Of--> stand -> 1\n",
            "Frequency Of--> tri -> 8\n",
            "Frequency Of--> exist -> 4\n",
            "Frequency Of--> pas -> 1\n",
            "Frequency Of--> absent -> 1\n",
            "Frequency Of--> act -> 11\n",
            "Frequency Of--> janesvil -> 2\n",
            "Frequency Of--> apprehend -> 1\n",
            "Frequency Of--> fact -> 4\n",
            "Frequency Of--> might -> 3\n",
            "Frequency Of--> alia -> 1\n",
            "Frequency Of--> indemn -> 1\n",
            "Frequency Of--> construct -> 3\n",
            "Frequency Of--> sawyer -> 2\n",
            "Frequency Of--> latter -> 3\n",
            "Frequency Of--> bank -> 1\n",
            "Frequency Of--> mass -> 1\n",
            "Frequency Of--> concern -> 1\n",
            "Frequency Of--> forbid -> 1\n",
            "Frequency Of--> upon -> 11\n",
            "Frequency Of--> cohen -> 2\n",
            "Frequency Of--> cost -> 1\n",
            "Frequency Of--> consequ -> 1\n",
            "Frequency Of--> design -> 1\n",
            "Frequency Of--> interest -> 5\n",
            "Frequency Of--> absenc -> 2\n",
            "Frequency Of--> whether -> 7\n",
            "Frequency Of--> yet -> 2\n",
            "Frequency Of--> confirm -> 1\n",
            "Frequency Of--> twenty-second -> 1\n",
            "Frequency Of--> type -> 2\n",
            "Frequency Of--> note -> 1\n",
            "Frequency Of--> view -> 2\n",
            "Frequency Of--> cha -> 1\n",
            "Frequency Of--> pay -> 2\n",
            "Frequency Of--> seiz -> 3\n",
            "Frequency Of--> short -> 1\n",
            "Frequency Of--> totem -> 1\n",
            "Frequency Of--> vari -> 1\n",
            "Frequency Of--> doubtless -> 1\n",
            "Frequency Of--> acquiesc -> 1\n",
            "Frequency Of--> dec -> 1\n",
            "Frequency Of--> order -> 1\n",
            "Frequency Of--> far -> 1\n",
            "Frequency Of--> injuri -> 1\n",
            "Frequency Of--> grow -> 17\n",
            "Frequency Of--> prohibit -> 3\n",
            "Frequency Of--> condit -> 4\n",
            "Frequency Of--> first -> 3\n",
            "Frequency Of--> destruct -> 1\n",
            "Frequency Of--> must -> 4\n",
            "Frequency Of--> american -> 1\n",
            "Frequency Of--> crop -> 3\n",
            "Frequency Of--> con -> 2\n",
            "Frequency Of--> result -> 3\n",
            "Frequency Of--> inhibit -> 2\n",
            "Frequency Of--> gather -> 22\n",
            "Frequency Of--> ground -> 2\n",
            "Frequency Of--> growl -> 1\n",
            "Frequency Of--> debt -> 2\n",
            "Frequency Of--> enter -> 3\n",
            "Frequency Of--> reserv -> 1\n",
            "Frequency Of--> foot -> 4\n",
            "Frequency Of--> oat -> 1\n",
            "Frequency Of--> j -> 3\n",
            "Frequency Of--> verb -> 1\n",
            "Frequency Of--> cite -> 13\n",
            "Frequency Of--> supra -> 1\n",
            "Frequency Of--> loss -> 1\n",
            "Frequency Of--> autauga -> 1\n",
            "Frequency Of--> retrospect -> 1\n",
            "Frequency Of--> proprieti -> 1\n",
            "Frequency Of--> rep -> 16\n",
            "Frequency Of--> case -> 24\n",
            "Frequency Of--> whenev -> 1\n",
            "Frequency Of--> nake -> 1\n",
            "Frequency Of--> sell -> 5\n",
            "Frequency Of--> chitti -> 1\n",
            "Frequency Of--> dri -> 1\n",
            "Frequency Of--> charg -> 4\n",
            "Frequency Of--> thorough -> 1\n",
            "Frequency Of--> owe -> 1\n",
            "Frequency Of--> bond -> 1\n",
            "Frequency Of--> juri -> 5\n",
            "Frequency Of--> protect -> 1\n",
            "Frequency Of--> elliott -> 1\n",
            "Frequency Of--> al -> 1\n",
            "Frequency Of--> adduc -> 2\n",
            "Frequency Of--> time -> 12\n",
            "Frequency Of--> deliv -> 2\n",
            "Frequency Of--> exert -> 1\n",
            "Frequency Of--> burton -> 1\n",
            "Frequency Of--> yield -> 1\n",
            "Frequency Of--> grain -> 1\n",
            "Frequency Of--> perhap -> 1\n",
            "Frequency Of--> dollar -> 3\n",
            "Frequency Of--> employ -> 1\n",
            "Frequency Of--> lee -> 1\n",
            "Frequency Of--> prevail -> 1\n",
            "Frequency Of--> perform -> 1\n",
            "Frequency Of--> document -> 1\n",
            "Frequency Of--> subject-matt -> 1\n",
            "Frequency Of--> away -> 4\n",
            "Frequency Of--> inquiri -> 2\n",
            "Frequency Of--> gener -> 2\n",
            "Frequency Of--> control -> 1\n",
            "Frequency Of--> virtu -> 1\n",
            "Frequency Of--> forbidden -> 2\n",
            "Frequency Of--> unless -> 1\n",
            "Frequency Of--> referenc -> 1\n",
            "Frequency Of--> soon -> 1\n",
            "Frequency Of--> prevent -> 3\n",
            "Frequency Of--> presid -> 1\n",
            "Frequency Of--> statu -> 1\n",
            "Frequency Of--> june -> 1\n",
            "Frequency Of--> plantat -> 1\n",
            "Frequency Of--> approv -> 1\n",
            "Frequency Of--> proposit -> 2\n",
            "Frequency Of--> levi -> 11\n",
            "Frequency Of--> summer -> 3\n",
            "Frequency Of--> claimant -> 23\n",
            "Frequency Of--> trover -> 3\n",
            "Frequency Of--> co -> 1\n",
            "Frequency Of--> give -> 7\n",
            "Frequency Of--> bacon -> 1\n",
            "Frequency Of--> vest -> 3\n",
            "Frequency Of--> prioriti -> 1\n",
            "Frequency Of--> collier -> 1\n",
            "Frequency Of--> suspend -> 3\n",
            "Frequency Of--> murphi -> 1\n",
            "Frequency Of--> fourteen -> 2\n",
            "Frequency Of--> good -> 5\n",
            "Frequency Of--> austin -> 2\n",
            "Frequency Of--> anomali -> 1\n",
            "Frequency Of--> destroy -> 1\n",
            "Frequency Of--> whitlock -> 1\n",
            "Frequency Of--> thirti -> 1\n",
            "Frequency Of--> white -> 2\n",
            "Frequency Of--> fa -> 1\n",
            "Frequency Of--> twenti -> 1\n",
            "Frequency Of--> thu -> 1\n",
            "Frequency Of--> jun -> 1\n",
            "Frequency Of--> we -> 1\n",
            "Frequency Of--> say -> 3\n",
            "Frequency Of--> languag -> 1\n",
            "Frequency Of--> r -> 1\n",
            "Frequency Of--> montgomeri -> 2\n",
            "Frequency Of--> previou -> 2\n",
            "Frequency Of--> amount -> 1\n",
            "Frequency Of--> nov -> 2\n",
            "Frequency Of--> w -> 3\n",
            "Frequency Of--> parti -> 1\n",
            "Frequency Of--> assigne -> 1\n",
            "Frequency Of--> recov -> 1\n",
            "Frequency Of--> chit -> 1\n",
            "Frequency Of--> cow -> 2\n",
            "Frequency Of--> six -> 1\n",
            "Frequency Of--> receiv -> 1\n",
            "Frequency Of--> immateri -> 1\n",
            "Frequency Of--> consist -> 1\n",
            "Frequency Of--> undertak -> 1\n",
            "Frequency Of--> jancey -> 1\n",
            "Frequency Of--> creat -> 1\n",
            "Frequency Of--> cotton -> 14\n",
            "Frequency Of--> agreement -> 1\n",
            "Frequency Of--> contract -> 16\n",
            "Frequency Of--> matter -> 3\n",
            "Frequency Of--> editor -> 2\n",
            "Frequency Of--> sacrific -> 2\n",
            "Frequency Of--> partner -> 1\n",
            "Frequency Of--> dane -> 1\n",
            "Frequency Of--> ree -> 1\n",
            "Frequency Of--> titl -> 5\n",
            "Frequency Of--> word -> 2\n",
            "Frequency Of--> person -> 4\n",
            "Frequency Of--> power -> 1\n",
            "Frequency Of--> expo -> 1\n",
            "Frequency Of--> transfer -> 1\n",
            "Frequency Of--> present -> 2\n",
            "Frequency Of--> mention -> 4\n",
            "Frequency Of--> convers -> 3\n",
            "Frequency Of--> purchas -> 1\n",
            "Frequency Of--> reuter -> 1\n",
            "Frequency Of--> principl -> 2\n",
            "Frequency Of--> tuskaloosa -> 1\n",
            "Frequency Of--> decid -> 2\n",
            "Frequency Of--> citat -> 5\n",
            "Frequency Of--> maintain -> 1\n",
            "Frequency Of--> situat -> 1\n",
            "Frequency Of--> number -> 1\n",
            "Frequency Of--> withdrawn -> 1\n",
            "Frequency Of--> lost -> 2\n",
            "Frequency Of--> corn -> 4\n",
            "Frequency Of--> futur -> 3\n",
            "Frequency Of--> sum -> 1\n",
            "Frequency Of--> use -> 2\n",
            "Frequency Of--> farm -> 2\n",
            "Frequency Of--> hand -> 3\n",
            "Frequency Of--> assum -> 2\n",
            "Frequency Of--> immatur -> 3\n",
            "Frequency Of--> alston -> 1\n",
            "Frequency Of--> manner -> 1\n",
            "Frequency Of--> east -> 2\n",
            "Frequency Of--> smith -> 1\n",
            "Frequency Of--> intent -> 2\n",
            "Frequency Of--> wend -> 1\n",
            "Frequency Of--> undeni -> 1\n",
            "Frequency Of--> jul -> 1\n",
            "Frequency Of--> doubt -> 5\n",
            "Frequency Of--> reliev -> 1\n",
            "Frequency Of--> circuit -> 10\n",
            "Frequency Of--> am -> 1\n",
            "Frequency Of--> speci -> 1\n",
            "Frequency Of--> estat -> 1\n",
            "Frequency Of--> port -> 1\n",
            "Frequency Of--> chattel -> 4\n",
            "Frequency Of--> bona -> 1\n",
            "Frequency Of--> term -> 11\n",
            "Frequency Of--> defer -> 1\n",
            "Frequency Of--> unquestion -> 2\n",
            "Frequency Of--> eo -> 1\n",
            "Frequency Of--> sw -> 1\n",
            "Frequency Of--> follow -> 3\n",
            "Frequency Of--> seizur -> 2\n",
            "Frequency Of--> properti -> 10\n",
            "Frequency Of--> septemb -> 2\n",
            "Frequency Of--> et -> 1\n",
            "Frequency Of--> manson -> 1\n",
            "Frequency Of--> half -> 1\n",
            "Frequency Of--> author -> 2\n",
            "Frequency Of--> suffici -> 1\n",
            "Frequency Of--> take -> 4\n",
            "Frequency Of--> dig -> 4\n",
            "Frequency Of--> frequent -> 2\n",
            "Frequency Of--> soil -> 3\n",
            "Frequency Of--> discharg -> 1\n",
            "Frequency Of--> strict -> 1\n",
            "Frequency Of--> warehous -> 2\n",
            "Frequency Of--> truth -> 1\n",
            "Frequency Of--> ask -> 3\n",
            "Frequency Of--> particular -> 1\n",
            "Frequency Of--> set -> 1\n",
            "Frequency Of--> secur -> 2\n",
            "Frequency Of--> interf -> 1\n",
            "Frequency Of--> wash -> 1\n",
            "Frequency Of--> declar -> 3\n",
            "Frequency Of--> back -> 1\n",
            "Frequency Of--> g -> 1\n",
            "Frequency Of--> feel -> 1\n",
            "Frequency Of--> write -> 3\n",
            "Frequency Of--> necessari -> 3\n",
            "Frequency Of--> upward -> 2\n",
            "Frequency Of--> suffer -> 2\n",
            "Frequency Of--> p -> 3\n",
            "Frequency Of--> former -> 2\n",
            "Frequency Of--> jan -> 3\n",
            "Frequency Of--> court -> 20\n",
            "Frequency Of--> l -> 1\n",
            "Frequency Of--> prove -> 3\n",
            "Frequency Of--> x -> 1\n",
            "Frequency Of--> hon -> 6\n",
            "Frequency Of--> seventy-seven -> 1\n",
            "Frequency Of--> comment -> 1\n",
            "Frequency Of--> pretend -> 1\n",
            "Frequency Of--> tennesse -> 1\n",
            "Frequency Of--> appear -> 2\n",
            "Frequency Of--> intend -> 1\n",
            "Frequency Of--> whippl -> 4\n",
            "Frequency Of--> valueth -> 1\n",
            "Frequency Of--> accord -> 1\n",
            "Frequency Of--> trespass -> 1\n",
            "Frequency Of--> fidei -> 1\n",
            "Frequency Of--> coat -> 1\n",
            "Frequency Of--> save -> 1\n",
            "Frequency Of--> convey -> 3\n",
            "Frequency Of--> four -> 1\n",
            "Frequency Of--> explicit -> 1\n",
            "Frequency Of--> endors -> 1\n",
            "Frequency Of--> consid -> 4\n",
            "Frequency Of--> insist -> 2\n",
            "Frequency Of--> equiti -> 2\n",
            "Frequency Of--> execut -> 1\n",
            "Frequency Of--> abey -> 1\n",
            "Frequency Of--> debtor -> 9\n",
            "Frequency Of--> substanti -> 1\n",
            "Frequency Of--> mercantil -> 1\n",
            "Frequency Of--> major -> 1\n",
            "Frequency Of--> gave -> 1\n",
            "Frequency Of--> origin -> 1\n",
            "Frequency Of--> depth -> 2\n",
            "Frequency Of--> land -> 1\n",
            "Frequency Of--> trial -> 3\n",
            "Frequency Of--> involv -> 3\n",
            "Frequency Of--> year -> 3\n",
            "Frequency Of--> compet -> 1\n",
            "Frequency Of--> nulliti -> 1\n",
            "Frequency Of--> ten -> 1\n",
            "Frequency Of--> given -> 2\n",
            "Frequency Of--> provid -> 1\n",
            "Frequency Of--> down -> 1\n",
            "Frequency Of--> object -> 1\n",
            "Frequency Of--> talk -> 2\n",
            "Frequency Of--> invest -> 1\n",
            "Frequency Of--> viz -> 1\n",
            "Frequency Of--> noth -> 3\n",
            "Frequency Of--> engag -> 1\n",
            "Frequency Of--> banish -> 1\n",
            "Frequency Of--> all -> 2\n",
            "Frequency Of--> disput -> 1\n",
            "Frequency Of--> faith -> 3\n",
            "Frequency Of--> allen -> 3\n",
            "Frequency Of--> singular -> 1\n",
            "Frequency Of--> unrip -> 1\n",
            "Frequency Of--> weakli -> 1\n",
            "Frequency Of--> rather -> 1\n",
            "Frequency Of--> product -> 1\n",
            "Frequency Of--> potent -> 1\n",
            "Frequency Of--> see -> 1\n",
            "Frequency Of--> submit -> 2\n",
            "Frequency Of--> govern -> 1\n",
            "Frequency Of--> temporari -> 1\n",
            "Frequency Of--> quitclaim -> 1\n",
            "Frequency Of--> n -> 1\n",
            "Frequency Of--> acr -> 1\n",
            "Frequency Of--> reason -> 2\n",
            "Frequency Of--> dispos -> 4\n",
            "Frequency Of--> ever -> 1\n",
            "Frequency Of--> john -> 9\n",
            "Frequency Of--> infer -> 1\n",
            "Frequency Of--> thereaft -> 1\n",
            "Frequency Of--> learn -> 1\n",
            "Frequency Of--> plant -> 9\n",
            "Frequency Of--> syrup -> 2\n",
            "Frequency Of--> voluntarili -> 1\n",
            "Frequency Of--> ordinari -> 1\n",
            "Frequency Of--> subsequ -> 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2fcN2BrEFER",
        "colab_type": "code",
        "outputId": "d2c9cf3e-627d-4b26-e08f-ec11fb469fb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "#1.4\n",
        "from itertools import chain\n",
        "from nltk import ngrams\n",
        "new=[]\n",
        "for i in rep_rem:\n",
        "  token_s = word_tokenize(i)\n",
        "  new.append(token_s)\n",
        "list2 = [x for x in new if x != []]\n",
        "a=list(itertools.chain.from_iterable(list2))\n",
        "#one =ngrams(a, n)\n",
        "one=ngrams(a,1)\n",
        "print('Top 10 one grams-')\n",
        "Counter(one).most_common(10)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 one grams-\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('case',), 24),\n",
              " (('claimant',), 23),\n",
              " (('gather',), 22),\n",
              " (('right',), 21),\n",
              " (('court',), 20),\n",
              " (('law',), 18),\n",
              " (('grow',), 17),\n",
              " (('contract',), 16),\n",
              " (('rep',), 16),\n",
              " (('ala',), 15)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxRhT3nWTITt",
        "colab_type": "code",
        "outputId": "253f25d3-fefa-4d4d-eb2f-de082c76ec7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "#1.4\n",
        "bgs = nltk.bigrams(a)\n",
        "fdist = nltk.FreqDist(bgs)\n",
        "print(\"Top 10 Bigrams-\")\n",
        "(fdist.most_common(10))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 Bigrams-\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('circuit', 'court'), 9),\n",
              " (('case', 'cite'), 8),\n",
              " (('common', 'law'), 7),\n",
              " (('term', 'case'), 7),\n",
              " (('john', 'rep'), 6),\n",
              " (('ala', 'ala'), 6),\n",
              " (('tri', 'hon'), 6),\n",
              " (('fieri', 'facial'), 5),\n",
              " (('whippl', 'foot'), 4),\n",
              " (('appeal', 'circuit'), 4)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJC-Q9ygPqae",
        "colab_type": "code",
        "outputId": "5986dea7-58e5-41bb-a62e-52e74c5189e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "#1.4\n",
        "print(\"Top 10 tri grams\")\n",
        "tri = nltk.trigrams(a)\n",
        "fdist = nltk.FreqDist(tri)\n",
        "fdist.most_common(10)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 tri grams\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('term', 'case', 'cite'), 5),\n",
              " (('appeal', 'circuit', 'court'), 4),\n",
              " (('error', 'circuit', 'court'), 3),\n",
              " (('ala', 'ala', 'trover'), 3),\n",
              " (('ala', 'trover', 'convers'), 3),\n",
              " (('jan', 'term', 'case'), 3),\n",
              " (('writ', 'error', 'circuit'), 2),\n",
              " (('circuit', 'court', 'summer'), 2),\n",
              " (('interest', 'vest', 'posse'), 2),\n",
              " (('vest', 'posse', 'either'), 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBiC4E_kefvV",
        "colab_type": "text"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QJ-UwCenvN",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. \n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSv6fVhOfFmv",
        "colab_type": "code",
        "outputId": "7d2e8d73-296b-43bc-8c53-0d6a2a27bb8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Write your code here\n",
        "import re\n",
        "ip = \"260.08.094.109\"\n",
        "output1 = re.sub(r'\\b0+(\\d)', r'\\1', ip)\n",
        "output1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'260.8.94.109'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRjaHzrfKAy",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence.\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xdJpDx9gjbX",
        "colab_type": "code",
        "outputId": "ae5c1e76-450c-45a2-9544-34e8a638790c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Write your code here\n",
        "import re\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\n",
        "output2=re.findall(r'2\\d\\d\\d', sentence)\n",
        "output2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2010', '2010', '2019']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    }
  ]
}